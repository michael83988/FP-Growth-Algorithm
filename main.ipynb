FP-Growth algorithm
此為Association Rule Analysis中的一個演算法，透過建立 FP Tree，並遍歷FP Tree來形成frequent itemset，避免apriori algorithm中需要不斷查詢整個raw data的步驟(很耗時)。FP-Growth algorithm只需要對raw data進行2次的掃描，大大的降低執行所需的時間，提升執行效率。

FP-Growth algorithm可拆解為:

掃描整個raw data，對所有元素的出現次數做統計，並根據出現次數由大到小排序建立出一個參考的list(frequent list)
將raw data中的每一筆資料按照frequent list做過濾與排序(由出現次數最多到最少)
建立FP Tree(以root node為開頭)
參考frequent list，以每一個在frequent list中的元素為header，從FP Tree中找出相對應的frequent itemset(得出來的itemset稱為conditional pattern base, CPB)
以得到的conditional pattern base當作新的raw data，重複步驟3,4，直到CPB為None為止
從root node開始，將每個header連接起來，及為透過FP-Growth algorithm找出的frequent itemsets
定義所需的變數


import pandas as pd
import numpy as np
import inspect

#support = 0.3
min_support = 3  #for test
confidence = 0.6
#frequent_itemsets_list = []  #儲存最後frequent itemsets的list


class Node:
    #建構式
    def __init__(self, item = None, count=1, parent=None, children=[]):
        self.__item = item
        self.__count = count
        self.__parent = parent
        self.__children = children
        
    def append_child(self, child):
        self.__children.append(child)
    
    def add_count(self):
        self.__count = self.__count + 1
    
    def get_item(self):
        return self.__item
    
    def get_count(self):
        return self.__count
    
    def get_parent(self):
        return self.__parent
    
    def get_children(self):
        return self.__children
        
#test -> OK!
"""
n0 = Node('n0')
n1 = Node(item='n1', parent=n0)
n0.append_child(n1)
n2 = Node(item='n2', parent=n0)
n0.append_child(n2)
n3 = Node(item='n3', parent=n1)
n1.append_child(n3)
print(n0.get_item())
print(n0.get_count())
print(n1.get_count())
print(n1.get_parent().get_children())
print(n3.get_parent().get_parent().get_item())
"""


主要的function
#從raw data中統計各個item的數量
def counting(df):
    #print(df.columns.dtype)
    #global init_item_count_dict
    count_dict = {}
    for i in range(len(df.index)):
        tmp_row = df.iloc[i]
        for j in range(len(tmp_row.index)):
            #print(type(tmp_row[j]))
            #每個nan皆為獨特的存在! 是float型態 -> 其他數字自動被當作float
            #if(tmp_row[j] not in list(count_dict.keys()) and ((type(tmp_row[j]) is not (str or object) and not np.isnan(tmp_row[j])) or (type(tmp_row[j]) is (str or object)))):
            if(tmp_row[j] not in list(count_dict.keys()) and not pd.isnull(tmp_row[j])):
                count_dict[tmp_row[j]] = 1
            elif(tmp_row[j] in list(count_dict.keys())):
                count_dict[tmp_row[j]] = count_dict[tmp_row[j]] + 1
            else:
                #print(tmp_row[j])
                #print('A NaN value was read ...')
                continue
    
    return count_dict         
    
"""
#過濾掉小於support的items，並由大到小排序
def is_over_support_items(obj, df):
    filtered_obj = {}
    
    global support
    #global min_support  #for test
    
    #過濾
    #print('有用support過濾')
    for k, v in obj.items():
        if(supp([k], df) >= support):
            #print('support值: ', support)
            filtered_obj[k] = v
    
    #由大到小排序
    ordered_items = [item[0] for item in sorted(filtered_obj.items(), key = lambda x:x[1], reverse = True)]
    return ordered_items
"""    
    
#過濾掉小於support的items，並由大到小排序(min support)
def is_over_support_items_min_support(obj, df):
    filtered_obj = {}
    
    #global support
    global min_support  #for test
    
    #過濾
    for k, v in obj.items():
        #if(supp(k, df) >= support):
        if(v >= min_support):
            filtered_obj[k] = v
    
    #由大到小排序
    ordered_items = [item[0] for item in sorted(filtered_obj.items(), key = lambda x:x[1], reverse = True)]
    return ordered_items

#計算support
def supp(item, df):
    result = {}
    item = tuple(item)
    total_row = len(df.index)
    for i in range(total_row):
        temp_row = df.iloc[i]
        #print(temp_row)
        
        if(all(k in temp_row.values for k in item) and (tuple(item) not in list(result.keys()))):
            result[tuple(item)] = 1
            #print('有建立')
        elif(all(k in temp_row.values for k in item) and (tuple(item) in list(result.keys()))):
            result[tuple(item)] = result[tuple(item)] + 1
            #print('加1')
        else:
            #print('不符合')
            #print([k for k in key])
            continue
    
    #print('內部統計結果dict: ',inner_temp_dict)
    #print('supp中的result: ', result)
    return result[tuple(item)] / total_row if tuple(item) in list(result.keys()) else 0
    

#計算confidence
def conf(x, itemset, df):
#     print(df)
#     print('分子: ', itemset, supp(list(itemset), df))
#     print('分母: ', x, supp(list(x), df))
    return supp(list(itemset), df) / supp(list(x), df) 

#計算lift
def lift(x, y, item, df):
    return supp(list(item), df) / (supp(list(x), df) * supp(list(y), df))




#將raw data的每一筆資料重新排序(按照ordered_items的順序)
def reorder_raw_data(ordered_items, df):
    datas = []
    for i in range(len(df.index)):
        tmp_row_values = df.iloc[i].values
        tmp_data = []
        for j in range(len(ordered_items)):
            if(ordered_items[j] in tmp_row_values):
                tmp_data.append(ordered_items[j])
        datas.append(tmp_data.copy()) 
    
    return datas


#建立FP Tree
def fp_tree(reordered_raw_data):
    #print(root is not None)
    root = Node(children = [])
    current_node = root
    #print(root.get_children())  #已經有children了?!?! 可能是因為children是list型態，會沿用同樣的位址?
    #print('children: ', id(root.get_children()))
    #print('root: ', id(root))
    #print('root count: ', root.get_count())
    #tmp_node = None
    
    for i in range(len(reordered_raw_data)):
        tmp_row = reordered_raw_data[i]
        current_node = root
        
        for j in range(len(tmp_row)):
            #print('第', i + j, '個點')  
            #初始狀態: 只有root
            #在根結點 && children中沒有一樣的
            if(current_node.get_item() is None and all(tmp_row[j] != node.get_item() for node in current_node.get_children())):
                tmp_node = Node(item = tmp_row[j], parent = current_node, children = [])
                current_node.append_child(tmp_node)
                current_node = tmp_node
                #print('在root加入新成員到children, current node移到新建立的點')
                

            #在根結點 && children中有一樣的
            elif(current_node.get_item() is None and any(tmp_row[j] == node.get_item() for node in current_node.get_children())):
                same_item = [child for child in current_node.get_children() if child.get_item() == tmp_row[j]][0]
                same_item.add_count()
                current_node = same_item
                #print('在root的children中找到一樣的, 該結點加一, current node移到該點')
                

            #在第n個結點(同一個row的資料才會走這條判斷!) && 與該結點一樣 -> 應該不會發生?!
            elif(current_node.get_item() == tmp_row[j]):
                current_node.add_count()
                #print('與該結點相同，count加一, Majaja')
                

            #在第n個結點(同一個row的資料才會走這條判斷!) && 與該結點不一樣 -> children中有
            elif(current_node.get_item() != tmp_row[j] and any(tmp_row[j] == node.get_item() for node in current_node.get_children())):
                same_item = [child for child in current_node.get_children() if child.get_item() == tmp_row[j]][0]
                same_item.add_count()
                current_node = same_item
                #print('在ni的children中找到一樣的 -> count加一')

            #在第n個結點 && 與該結點不一樣 -> children中沒有
            elif(current_node.get_item() != tmp_row[j] and all(tmp_row[j] != node.get_item() for node in current_node.get_children())):
                tmp_node = Node(item = tmp_row[j], parent = current_node, children = [])
                current_node.append_child(tmp_node)
                current_node = tmp_node
                #print('在ni的children中都沒有 -> 新增一個結點')


            else:
                print('出事啦阿伯?!')
                
                    
                
            
                
    
    return root


#從FP-Tree中得出conditional pattern base
def get_CPB(ordered_items, root):
    CPB = {}
    tmp_itemset = []  #用來存特定item name在特定節點所拉出來的itemset (然後再轉換成tuple)
    tmp_itemset_list = []  #存特定item name所對應的所有itemset (裝每個tuple)
    exploded_tree = []  #用來存放爆開來的tree
    
    
    # tree是從root開始
    #先將tree爆開，然後把符合ordered_items中的item name的節點所拉出來的itemset暫存到tem_itemset, 然後再存到CPB中
    current_node = root
    while True:
        if(len(current_node.get_children()) != 0):
            current_node = current_node.get_children()[0]
            #print('走1')
        elif(current_node.get_item() is not None and len(current_node.get_children()) == 0):
            current_node = current_node.get_parent()
            exploded_tree.append(current_node.get_children().pop(0))
            #print('走2')
        elif(current_node.get_item() is None and len(current_node.get_children()) == 0):
            #print('走3')
            break
        else:
            print("Something wrong?")
            break
    
    #根據ordered_items的item, 去找出各自對應的CPB; prev_item為特定item前面的item
    for item in ordered_items:
        #tmp_list: 存放所有特定item的node
        tmp_list = [prev_item for prev_item in exploded_tree if prev_item.get_item() == item]
        #print(tmp_list)  #[]? -> OK
        
        
        #從tmp_list逐一拉出itemset
        for spec_item in tmp_list:
            count = spec_item.get_count()
            current_node = spec_item.get_parent()
            #spec_item = spec_item.get_parent()
            
            while(current_node.get_item() is not None):
                tmp_itemset.insert(0, current_node.get_item())
                current_node = current_node.get_parent()
            
            #將tuple加到tmp_itemset_list中 
            for _ in range(count):
                
                #排除空集合
                if(tmp_itemset != list()):
                    tmp_itemset_list.append(tuple(tmp_itemset.copy())) 
                #print('Majaja')
            
            #清空tmp_itemset
            tmp_itemset.clear()
            
            
        #將tmp_itemset_list加到CPB中
        CPB[item] = tmp_itemset_list.copy()
        tmp_itemset_list.clear()
    
    return CPB

"""
#運作FP-Growth algorithm -> Something wrong?! 不符合FP-Growth algorithm的方法 -> 應該停用!
def FP_Growth(df, freq_list = [], base = set()):
    
    #freq_list: 儲存最終結果的list:
    #base: 用來找出各個從CPB找出的itemset, 避免掉有重複的itemset加到freq_list中
    
    
    
    
    #從raw data中統計各個item的數量
    statistics = counting(df)
    
    
    #找出排序過後(由大到小)的header
    header = is_over_support_items(statistics, df)
    #print('header: ', header)
    
    #將raw data (or CPB的data)按照header重新排序
    reordered_data = reorder_raw_data(header, df)
    #print('reordered data: ', reordered_data)
    
    #建立FP Tree
    root = fp_tree(reordered_data)
    #print('root: ', root)
    
    #check
    
    for child in root.get_children():
        print(child.get_item())
        print(child.get_count())
    
    
    #建立CPB
    CPB = get_CPB(header, root)
    #print('CPB: ', CPB)
    
    
    
    #從CPB中取出frequent itemset
    for head in header:
        new_base = base.copy()
        new_base.add(head)
        
        #global frequent_itemsets_list
        freq_list.append(new_base)
        
        #透過當前的head從CPB取出對應的conditional dataset
        data = [list(item) for item in CPB[head]]
        #print('temp data of {}: {}'.format(head, data))
        
        #若為空，表示沒有頻繁項集了
        #遞迴找出frequent itemseta
        if(len(data) > 0):
            #print('Recursive calling')
            new_dataset = pd.DataFrame(data)
            FP_Growth(new_dataset, freq_list, new_base)
        
        
    #如何將結果(frequent_itemsets_list)回傳??
    return freq_list #不行, why? -> ok!
    
"""    
    
#運作FP-Growth algorithm(over min support版本)
def FP_Growth_Min(df, freq_list = [], base = set()):
    
    #freq_list: 儲存最終結果的list:
    #base: 用來找出各個從CPB找出的itemset, 避免掉有重複的itemset加到freq_list中
    
    
    if(inspect.stack()[1][3] == "<module>"):
        print('開始執行FP-Growth Algorithm ...')
        freq_list.clear()  #需清空freq_list讓重複執行時，前面的資料可以清除
    
    
    #test of function caller name
    #print('function name: ', inspect.stack()[1][3])
    
    #從raw data中統計各個item的數量
    statistics = counting(df)
    
    
    #找出排序過後(由大到小)的header
    header = is_over_support_items_min_support(statistics, df)
    #print('header: ', header)
    
    #將raw data (or CPB的data)按照header重新排序
    reordered_data = reorder_raw_data(header, df)
    #print('reordered data: ', reordered_data)
    
    #建立FP Tree
    root = fp_tree(reordered_data)
    #print('root: ', root)
    
    #check
    """
    for child in root.get_children():
        print(child.get_item())
        print(child.get_count())
    """
    
    #建立CPB
    CPB = get_CPB(header, root)
    #print('CPB: ', CPB)
    
    
    
    #從CPB中取出frequent itemset
    for head in header:
        new_base = base.copy()
        new_base.add(head)
        
        #global frequent_itemsets_list
        freq_list.append(new_base)
        
        #透過當前的head從CPB取出對應的conditional dataset
        data = [list(item) for item in CPB[head]]
        #print('temp data of {}: {}'.format(head, data))
        
        #若為空，表示沒有頻繁項集了
        #遞迴找出frequent itemseta
        if(len(data) > 0):
            #print('Recursive calling')
            new_dataset = pd.DataFrame(data)
            FP_Growth_Min(new_dataset, freq_list, new_base)
        
        
    #如何將結果(frequent_itemsets_list)回傳??
    if(inspect.stack()[1][3] == "<module>"):
        print('FP-Growth Algorithm執行完成！')
        print('開始confidence的過濾 ...')
        result = filter_with_conf(freq_list, df)
        return result
        
    #return freq_list #不行, why? -> ok!    

"""
def clear_param(*args):
    global init_item_count_dict
    init_item_count_dict = {}
    
    global ordered_frequent_item_list
    ordered_frequent_item_list = []
"""    

#定義組合的function -> 用於產生所有的S組合
#lt: 輸入進來的list; n: 組合的元素有n個
def combine(lt, n):
    #print('呼叫combine')
    answers = []
    tmp_item = [0] * n
    def next_it(lt_lbound, ni):
        if(ni == n):
            answers.append(tmp_item.copy())
            return
        for li in range(lt_lbound, len(lt)):
            tmp_item[ni] = lt[li]
            next_it(li + 1, ni + 1)
    next_it(0, 0)
    return answers


#從frequent list中的itemset去找出>=confidence的組合，並計算其lift
def filter_with_conf(freq_list, df):
    #print('過濾掉小於confidence值的組合 ...')
    
    #找出每個frequent itemset的S
    combination = {}        
    #print('combination: ', combination)
    for item in freq_list:
        combination[tuple(item)] = list()
        for n in range(1, len(item)):
            #print('item: ', combination[item])
            combination[tuple(item)].extend(combine(tuple(item), n).copy())
    #print('組合結果: ', combination) 
    
    #根據S找出I-S，然後計算confidence
    pass_confidence = {}
    for k, v in combination.items():
        pass_confidence[k] = {}
        #print('v: ', v)
        for s in v:
            I_S = tuple(filter(lambda x:x not in s, k))
#             print('I_S: ', I_S)
#             print('s: ', s)
#             print('k: ', k)
            #initialize
            pass_confidence[k][tuple(s)] = [I_S, conf(s, k, df)]
    
    #print('計算confidence之後: ', pass_confidence)
    
    #濾掉 < confidence 的資料
    global confidence
    prepare_to_remove = []
    for k, v in pass_confidence.items():
        #print('k, v: ', k, v)
        
        #若v為empty, 不進入s, vi的循環 -> 需另外處理
        if(not bool(v)):
            prepare_to_remove.append([k, None])
        
        #針對v不為空，而其中某個s的confidence小於threshold值得處理
        for s, vi in v.items():
            #print('s, vi: ', s, vi)
            #去除confidence小於threshold值的數據(包含null的資料)
            if(vi[1] < confidence):
                
                prepare_to_remove.append([k, s])
                #pass_confidence[k].pop(s)
        
        #處理v為empty的情況         
        
                
    #pass_confidence[ks[0]].pop(ks[1]) for ks in prepare_to_remove
    for ks in prepare_to_remove:
        #print('ks: ', ks[0], ks[1])
        if(ks[1] is not None):
            pass_confidence[ks[0]].pop(ks[1])
        elif(ks[1] is None):
            pass_confidence.pop(ks[0])
        else:
            print("Something wrong when popping out value smaller than confidence's threshold")
    
    
    #print('過濾掉confidence太小之後: ', pass_confidence)
    
    #返回dataframe形式的result
    # frequent itemset / support / antecedent / consequent / confidence
    columns = ['frequent itemset', 'support', 'antecedent', 'consequent', 'confidence', 'lift']
    data = []
    for k, v in pass_confidence.items():
        for s, vi in v.items():
            #I_S = tuple(filter(lambda x:x not in s, k))
            tmp_row = [k, supp(k, df), s, vi[0], vi[1], lift(s, vi[0], k, df)]
            data.append(tmp_row)
    
    result_df = pd.DataFrame(data = data, columns = columns)
    print('透過confidence篩選完成!')
    return result_df



def set_param(sup = 10, con = 0.6):
#     global support
#     support = sup
    
    global min_support
    min_support = sup
    
    global confidence
    confidence = con
    
def show_param():
    #global support
    global min_support
    global confidence
    #print('support: ', support)
    print('min support: ', min_support)
    print('confidence: ', confidence)


將.ipynb轉成.py
#將.ipynb檔轉成.py檔 -> 以利其他python檔的import
!jupyter nbconvert --to script FPGrowth.ipynb
